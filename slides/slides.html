<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>The power of adjoints</title>

		<meta name="description" content="The power of adjoints">
		<meta name="author" content="Simon W Funke">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<b>Machine learning with expert systems</b>
					<br>or "The power of adjoints"
					<p>
						<small>9th May 2017<br><a href="http://simonfunke.com">Simon W Funke</a><br>Simula Research Laboratory</small>
					</p>
				</section>



				<section>
					<b>2016:</b> a machine beats the world's best human Go player
					<!-- . works well with much data<br>but little understanding -->
					<br>
					<!-- Im März 2016 schlug AlphaGo den Südkoreaner Lee Sedol, der als einer der weltbesten Profispieler angesehen wird --> 
					<img width="550" data-src="media/alphago.jpg">
					<!-- AlphaGo uses 30Million positions to train its AlphaGo model --> 
				</section>
				<section>
					The key concept of machine learning
					<br>
					<img width="100%" class="plain"  data-src="media/how_it_works_ml.svg">
					<br><small>Works best with lots of data</small>
				</section>




				<section>
					<b>2012:</b> NASA et al releases world's highest resolution ocean and sea-ice simulation
					<br>
					<iframe width="800" height="460" src="https://www.youtube.com/embed/CCmTY0PKGDs" frameborder="0" allowfullscreen></iframe> 
				</section>
	


				<section>
					The key concept of model based simulation
					<img width="100%" class="plain"  data-src="media/how_it_works_model.svg">
					<br><small>Works best with good problem knowledge</small>
				</section>

				<section>
					Machine learning 
					<br>+ Model based simulation 
					<br>= <b>Awesome?</b>

					<!-- More sensor data  -> model based simulation too restrictive -->
					<!-- Machine learning does not generalize well -->
					<!-- Most models are not complete -->
					<!-- Examples: Sub-model Weather/Ocean/Earth modeling -->
				</section>

				<section>
					Expert driven machine
					<img width="100%" class="plain"  data-src="media/how_it_works_expert_driven_ml.svg">
					<small>Application: Improving clinical decisions</small>
				</section>

				<section>
					Machine driven expert
					<img width="100%" class="plain"  data-src="media/how_it_works_ml_driven_expert.svg">
					<small>Application: Physics selection</small>
				</section>

				<section>
					Machine enhanced expert
					<img width="100%" class="plain"  data-src="media/how_it_works_ml_enhanced_expert.svg">
					<small>Application: Biomedical problems with unknown sub-physics</small>
				</section>

				<section>
					Expert enhanced machine
					<img width="100%" class="plain"  data-src="media/how_it_works_expert_enhanced_ml.svg">
					<small>Application: "Physics-aware machines"</small>
				</section>

				<section>
					<section>
					    A common mathematical framework
					</section>

					<section>
						The predictor function of a simple neural network<br>
						<img widht="100%" class="plain" data-src="media/nn-3-layer-network.png">
					</section>


					<section>
						The <b>predictor function</b> of a simple neural network<br>
						<br>
						<script type="math/tex">
						\begin{aligned}
						z_1 & = xW_1 + b_1 \\
						a_1 & = \tanh(z_1) \\
						z_2 & = a_1W_2 + b_2 \\
						\hat{y} &= \mathrm{softmax}(z_2)
						\end{aligned}
						</script>
						<br>
						<br>
						<small>
						$x$ is the input<br>
						$z_1$, $a_1$, $z_2$ are intermediate values<br>
						$\hat{y}$ is the output<br>
						$m=(W_1, b_1, W_2, b_2)$ are parameters.
						<br>
						<br>
						We can write the predictor in the general form $\mathcal N(x, \hat{y}; m) = 0$.
						</small>
					</section>

					<section>
					    <b>Training</b> a neural network: Given training data $(x_i, y_i), i=1..n$ find parameters that satisfy
						<br><br>
						<small>
						<script type="math/tex">
							\min_{m} \sum_{i=1}^n L(\hat{y}_i, y_i) + R(m) \\\text{subject to} \\
							\mathcal N(x_i, \hat{y}_i; m) = 0 \quad \forall i=1..n
						</script>
						</small>
						<br>
						<br>
						<small>
						$L$ is a loss function<br>
						$R$ is a regularization term<br>
						<br>
						</small>						
					</section>

					<section>
					    Demo
					    <small>(from http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)						</small>
					</section>

					<section>
						The <b>predictor function</b> of a simple spring model<br><br>

						<img width="20%" class="plain" style="position:absolute; left:-200px; top:100px;" data-src="media/spring.png">
						<script type="math/tex">
							 \frac{\mathrm{d}^2 x}{\mathrm{d}t^2} + 2D \frac{\mathrm{d} x}{\mathrm{d}t} + \omega^2 x = 0
						</script>
						<br><br>
						<small>
							$m = (\omega, D)$  are the parameters: angular frequency and damping factor<br>
							$x$ is its displacement from the mean position<br>
							$x(0) = 1$ and $x'(0) = -D$ are the initial conditions<br>
						
							<br>
							An explicit solution for this model is:
							<script type="math/tex">
								x(t) = \exp(-Dt) \cos(\omega t) 
							</script>
							<br><br>
							We can write the predictor in the general form $\mathcal S(x; m) = 0$.
						</small>
					</section>

					<section>
					    <b>Tuning</b> the spring model: Given observations $(t_i, x_i), i=1..n$ find parameters that satisfy
						<br><br>
						<small>
						<script type="math/tex">
							\min_{m} \sum_{i=1}^n L(\hat{x}_i, x_i) + R(m) \\\text{subject to} \\
							\mathcal S(t_i, \hat{x}_i; m) = 0 \quad \forall i=1..n
						</script>
						</small>
						<br>
						<br>
						<small>
						$L$ is a loss function<br>
						$R$ is a regularization term<br>
						<br>
						</small>						
						<!-- Summary</b><br>Both ML and model based simulation have a common mathematical formulation -->
					</section>

					<section>
					    Training a <b>Expert driven machine</b>
						<br>
						<img width="50%" class="plain"  data-src="media/how_it_works_expert_driven_ml.svg">
						<br>
						<small>
						Given observations $(t_i, y_i), i=1..n$ find parameters that satisfy<br>
						<br>
						<script type="math/tex">
							\min_{m_{\mathcal N}, m_{\mathcal S}} \sum_{i=1}^n L(\hat{y}_i, y_i) + R(m_{\mathcal N}, m_{\mathcal S}) \\\text{subject to} \\
							\mathcal N(t_i, \mathcal S(t_i, \hat{x}_i; m_{\mathcal S}), \hat{y}_i; m_{\mathcal N}) = 0 \quad \forall i=1..n
						</script>
						</small>
				</section>

				<section>
					    Training a <b>Machine driven expert</b>
						<br>
						<img width="50%" class="plain"  data-src="media/how_it_works_ml_driven_expert.svg">
						<br>
						<small>
						Given observations $(t_i, x_i), i=1..n$ find parameters that satisfy<br>
						<br>
						<script type="math/tex">
							\min_{m_{\mathcal S}, m_{\mathcal N}} \sum_{i=1}^n L(\hat{x}_i, x_i) + R(m_{\mathcal S}, m_{\mathcal N}) \\\text{subject to} \\
							\mathcal S(t_i, \mathcal N(t_i, \hat{y}_i; m_{\mathcal N}), \hat{x}_i; m_{\mathcal S}) = 0 \quad \forall i=1..n
						</script>
						</small>
				</section>

				<section>
					    <b>Solving the minimization problem needs:<br>1) Gradient-based optimization</b>
						<br>
						<img width="50%" class="plain"  data-src="media/Gradient_descent.svg">
				</section>				

				<section>
					    <b>Solving the minimization problem needs:
						<br>
						2. derivatives via the adjoint approach</b>
						<br><br>
						<small>
						Example:
						<script type="math/tex">
							\min j^T y \\ 
							\text{ subject to: } \\
							Ay = Bm
						</script>						
						<br>
						<br>
						The total derivative of $j^T y$ with respect to $m$ has the form:
						<img width="50%" class="plain"  data-src="media/visual_adjoint.jpg">
						<br>
						The adjoint approach solves $j^T A^{-1}$ first, making the computationally expense practically independent of the number of parameters.
						</small>
				</section>	
			</section>

			<section>
				    <b>Demo</b>
			</section>	


			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});

		</script>

	</body>
</html>
